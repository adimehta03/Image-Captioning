{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers.merge import add\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Add\n",
    "from keras.applications import InceptionV3\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(filename):\n",
    "  file = open(filename,'r')\n",
    "  text = file.read()\n",
    "  file.close()\n",
    "  return text\n",
    "\n",
    "def generate_img_captions(filename):\n",
    "  file = load_document(\"Flickr8k_text/Flickr8k.token.txt\")\n",
    "  captions = file.split('\\n')\n",
    "  descriptions ={}\n",
    "  for caption in captions[:-1]:\n",
    "    img, caption = caption.split('\\t')\n",
    "    if img[:-2] not in descriptions:\n",
    "      descriptions[img[:-2]] = []\n",
    "    descriptions[img[:-2]].append(caption)\n",
    "  return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_descriptions(descriptions,filename):\n",
    "    lines = list()\n",
    "    for key,caption_list in descriptions.items():\n",
    "        for caption in caption_list:\n",
    "            lines.append(key + '\\t' + caption)\n",
    "    image_descriptions = '\\n'.join(lines)\n",
    "    file = open(filename,'w')\n",
    "    file.write(image_descriptions)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images = \"Flickr8k_Dataset/Flicker8k_Dataset\"\n",
    "dataset_text = \"Flickr8k_text\"\n",
    "filename = dataset_text + \"/Flickr8k.token.txt\"\n",
    "descriptions = generate_img_captions(filename)\n",
    "print(len(descriptions))\n",
    "save_descriptions(descriptions,\"descriptions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = \"Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
    "train_data_img = open(train_dataset).read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InceptionV3(weights=\"imagenet\")\n",
    "new_input = model.input\n",
    "hidden_layer = model.layers[-2].output\n",
    "model_inc = Model(new_input,hidden_layer)\n",
    "model_inc.summary()\n",
    "#Pre-processing the images\n",
    "def preprocess_input(x):\n",
    "    x /= 255.\n",
    "    x -= 0.5\n",
    "    x *= 2.\n",
    "    return x\n",
    "def preprocess(image_path):\n",
    "    img = load_img(image_path, target_size=(299, 299))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(image):\n",
    "    image = preprocess(image)\n",
    "    temp_enc = model_vgg.predict(image)\n",
    "    temp_enc = np.reshape(temp_enc, temp_enc.shape[1])\n",
    "    return temp_enc\n",
    "features = {}\n",
    "for img in train_data_img:\n",
    "    if(len(img)>0):\n",
    "        features[img] = encode(\"Flickr8k_Dataset/Flicker8k_Dataset/\"+img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the extracted features\n",
    "with open(\"Features.p\", \"wb\") as f:\n",
    "    pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pickle.load(open(\"Features.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_descriptions = {}\n",
    "for image in descriptions:\n",
    "    if(image in train_data_img):\n",
    "        training_descriptions[image] = descriptions[image]\n",
    "training_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating vocabulary\n",
    "caps = []\n",
    "for key, val in training_descriptions.items():\n",
    "    for i in val:\n",
    "        caps.append('<start> ' + i + ' <end>')\n",
    "caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [i.split() for i in caps]\n",
    "unique = []\n",
    "for i in words:\n",
    "    unique.extend(i)\n",
    "unique = list(set(unique))\n",
    "with open(\"unique.p\", \"wb\") as pickle_d:\n",
    "    pickle.dump(unique, pickle_d)\n",
    "len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping the words in unique to a particular index\n",
    "word2index = {val:index for index, val in enumerate(unique)}\n",
    "index2word = {index:val for index, val in enumerate(unique)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating maximum length\n",
    "maximum_length = 0\n",
    "for c in caps:\n",
    "    c = c.split()\n",
    "    if len(c) > maximum_length:\n",
    "        maximum_length = len(c)\n",
    "maximum_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Vocabulary Size\n",
    "vocabulary_size = len(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('flickr8k_training_dataset.txt', 'w')\n",
    "f.write(\"image_id\\tcaptions\\n\")\n",
    "for key, val in training_descriptions.items():\n",
    "    for i in val:\n",
    "        f.write(key + \"\\t\" + \"<start> \" + i +\" <end>\" + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('flickr8k_training_dataset.txt', delimiter='\\t')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_epoch = 0\n",
    "for ca in caps:\n",
    "    samples_per_epoch += len(ca.split())-1\n",
    "samples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size = 32):\n",
    "        partial_caps = []\n",
    "        next_words = []\n",
    "        images = []\n",
    "        \n",
    "        df = pd.read_csv('flickr8k_training_dataset.txt', delimiter='\\t')\n",
    "        df = df.sample(frac=1)\n",
    "        iter = df.iterrows()\n",
    "        c = []\n",
    "        imgs = []\n",
    "        for i in range(df.shape[0]):\n",
    "            x = next(iter)\n",
    "            c.append(x[1][1])\n",
    "            imgs.append(x[1][0])\n",
    "        count = 0\n",
    "        while True:\n",
    "            for j, text in enumerate(c):\n",
    "                current_image = features[imgs[j]]\n",
    "                for i in range(len(text.split())-1):\n",
    "                    count+=1\n",
    "                    \n",
    "                    partial = [word2index[txt] for txt in text.split()[:i+1]]\n",
    "                    partial_caps.append(partial)\n",
    "                    \n",
    "                    # Initializing with zeros to create a one-hot encoding matrix\n",
    "                    # This is what we have to predict\n",
    "                    # Hence initializing it with vocab_size length\n",
    "                    n = np.zeros(vocabulary_size)\n",
    "                    # Setting the next word to 1 in the one-hot encoded matrix\n",
    "                    n[word2index[text.split()[i+1]]] = 1\n",
    "                    next_words.append(n)\n",
    "                    \n",
    "                    images.append(current_image)\n",
    "\n",
    "                    if count>=batch_size:\n",
    "                        next_words = np.asarray(next_words)\n",
    "                        images = np.asarray(images)\n",
    "                        partial_caps = pad_sequences(partial_caps, maxlen=maximum_length, padding='post')\n",
    "                        yield [[images, partial_caps], next_words]\n",
    "                        partial_caps = []\n",
    "                        next_words = []\n",
    "                        images = []\n",
    "                        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Design\n",
    "embedding_size=300\n",
    "\n",
    "image_model = Input(shape=(2048,))\n",
    "image_model1 = Dense(embedding_size, activation='relu')(image_model)\n",
    "image_model2 = RepeatVector(maximum_length)(image_model1) \n",
    "\n",
    "language_model = Input(shape=(maximum_length,))\n",
    "seq1 = Embedding(vocabulary_size,embedding_size,mask_zero=True)(language_model)\n",
    "seq2 = LSTM(256, return_sequences=True)(seq1)\n",
    "seq3 = TimeDistributed(Dense(embedding_size))(seq2)\n",
    "\n",
    "decoder1 = add([image_model2, seq3])\n",
    "decoder2 = Bidirectional(LSTM(256,return_sequences=False))(decoder1)\n",
    "outputs1 = Dense(vocabulary_size)(decoder2)\n",
    "outputs = Activation(\"softmax\")(outputs1)\n",
    "model = Model(inputs=[image_model,language_model],outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=1\n",
    "for i in range(10):\n",
    "    model.fit_generator(data_generator(batch_size=128), steps_per_epoch=samples_per_epoch/128, nb_epoch=1, verbose=1)\n",
    "model.save(\"Model-InceptionV\"+str(j)+\".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_captions(image):\n",
    "    start_word = [\"<start>\"]\n",
    "    while True:\n",
    "        par_caps = [word2index[i] for i in start_word]\n",
    "        par_caps = pad_sequences([par_caps], maxlen=maximum_length, padding='post')\n",
    "        e = encode(image)\n",
    "        preds = model.predict([np.array([e]), np.array(par_caps)])\n",
    "        word_pred = index2word[np.argmax(preds[0])]\n",
    "        start_word.append(word_pred)\n",
    "        \n",
    "        if word_pred == \"<end>\" or len(start_word) > maximum_length:\n",
    "            break\n",
    "            \n",
    "    return ' '.join(start_word[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_predictions(image, beam_index = 3):\n",
    "    start = [word2index[\"<start>\"]]\n",
    "    \n",
    "    start_word = [[start, 0.0]]\n",
    "    \n",
    "    while len(start_word[0][0]) < maximum_length:\n",
    "        temp = []\n",
    "        for s in start_word:\n",
    "            par_caps = pad_sequences([s[0]], maxlen=maximum_length, padding='post')\n",
    "            e = encode(image)\n",
    "            preds = model.predict([np.array([e]),np.array(par_caps)])\n",
    "            \n",
    "            word_preds = np.argsort(preds[0])[-beam_index:]\n",
    "            \n",
    "            # Getting the top <beam_index>(n) predictions and creating a \n",
    "            # new list so as to put them via the model again\n",
    "            for w in word_preds:\n",
    "                next_cap, prob = s[0][:], s[1]\n",
    "                next_cap.append(w)\n",
    "                prob += preds[0][w]\n",
    "                temp.append([next_cap, prob])\n",
    "                    \n",
    "        start_word = temp\n",
    "        # Sorting according to the probabilities\n",
    "        start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
    "        # Getting the top words\n",
    "        start_word = start_word[-beam_index:]\n",
    "    \n",
    "    start_word = start_word[-1][0]\n",
    "    intermediate_caption = [index2word[i] for i in start_word]\n",
    "\n",
    "    final_caption = []\n",
    "    \n",
    "    for i in intermediate_caption:\n",
    "        if i != '<end>':\n",
    "            final_caption.append(i)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    final_caption = ' '.join(final_caption[1:])\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_image = \"dog.jpeg\"\n",
    "Image.open(try_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Normal Max search:', predict_captions(try_image)) \n",
    "print ('Beam Search, k=3:', beam_search_predictions(try_image, beam_index=3))\n",
    "print ('Beam Search, k=5:', beam_search_predictions(try_image, beam_index=5))\n",
    "print ('Beam Search, k=7:', beam_search_predictions(try_image, beam_index=7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
